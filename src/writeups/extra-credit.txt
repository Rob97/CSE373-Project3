For the InsertionPreservingDictionary, I took a different approach than the extra credit guidelines suggested. In looking through the guides, it seemed to suggest that we were to implement the data structure using two arrays: one for the indices (references to entries) and one for the actual key value pairs. The problem with this is the remove method. You could theoretically just null out the item and just modify the behavior of the iterator to skip over the null items, but a problem arises that remove from the beginning of the list many times. In order to keep track of when insertions were made, you would have to continue adding to the end of the array even if there was empty spaces. If you got to the end of the array, you would have to either resize or shift everything over all at once which would change the runtime of the remove method to O(n) more often (as in you would have O(n) time when you need to resize as well as O(n) time when you have no spaces at the end of the array).

I then tried implementing the data structure with an array for indices and a linked double linked list for the key value pairs. This solution seemed very promising until I got down to testing efficiency. I originally chose secondary hashing as my probing method to avoid the affects of primary and secondary clustering, but in my testing, there was a big disadvantage to using any probing method. The stress test added key that had in order hash codes (1, 2, 3, 4, 5, etc). This is fine for inserting and removing because you would always go straight to the correct location without having to probe. However, the containsKey(K key) method was another story. If you tried to search for an item with a low valued hash code that did not exist in the dictionary you would end up checking a large proportion of the dictionary. This issue (similar to primary clustering) would normally be caused by linear probing, but this clustering had nothing to do with probing at all.

Because I wanted my structure to be as computationally efficient as possible, I thought of a different solution. I came up with the idea of reusing my ChainedHashDictionary and a modified linked list to make up my data structure. I would store the keys in a hash table with the value of the pair being a pointer to a node in a linked list. Normally removing from a linked list takes O(n) time because you have to iterate through all of the items in the link to see if it is even there (worst case). By also having the keys stored, I could know whether it was in the link just by checking the chained dictionary. Then I could just modify next and previous pointers of the node to delete it since I knew was in the chain.

The only disadvantage to this approach is the space complexity. Because there are more pointers stored in the linked list and the chained hash dictionary, it takes up considerably more space than array/array approach or array/linked-list approach would take up. However, we learned that there are advantages and disadvantages to every data structure we use and this one just happens to have superior time complexity but poor space complexity.
